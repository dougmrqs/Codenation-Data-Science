{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio 6\n",
    "\n",
    "Neste desafio, vamos praticar _feature engineering_, um dos processos mais importantes e trabalhosos de ML. Utilizaremos o _data set_ [Countries of the world](https://www.kaggle.com/fernandol/countries-of-the-world), que contém dados sobre os 227 países do mundo com informações sobre tamanho da população, área, imigração e setores de produção.\n",
    "\n",
    "> Obs.: Por favor, não modifique o nome das funções de resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Setup_ geral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import (KBinsDiscretizer,\n",
    "                                   OneHotEncoder,\n",
    "                                   StandardScaler)\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import (CountVectorizer,\n",
    "                                             TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algumas configurações para o matplotlib.\n",
    "#%matplotlib inline\n",
    "\n",
    "# from IPython.core.pylabtools import figsize\n",
    "\n",
    "\n",
    "# figsize(12, 8)\n",
    "\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\n",
    "    \"Country\", \"Region\", \"Population\", \"Area\", \"Pop_density\", \"Coastline_ratio\",\n",
    "    \"Net_migration\", \"Infant_mortality\", \"GDP\", \"Literacy\", \"Phones_per_1000\",\n",
    "    \"Arable\", \"Crops\", \"Other\", \"Climate\", \"Birthrate\", \"Deathrate\", \"Agriculture\",\n",
    "    \"Industry\", \"Service\"\n",
    "]\n",
    "\n",
    "countries.columns = new_column_names\n",
    "\n",
    "countries.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observações\n",
    "\n",
    "Esse _data set_ ainda precisa de alguns ajustes iniciais. Primeiro, note que as variáveis numéricas estão usando vírgula como separador decimal e estão codificadas como strings. Corrija isso antes de continuar: transforme essas variáveis em numéricas adequadamente.\n",
    "\n",
    "Além disso, as variáveis `Country` e `Region` possuem espaços a mais no começo e no final da string. Você pode utilizar o método `str.strip()` para remover esses espaços."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicia sua análise a partir daqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove espaços a mais\n",
    "df['Country'] = [country.strip() for country in df['Country']]\n",
    "df['Region'] = [region.strip() for region in df['Region']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(',','.', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = ['Pop_density', 'Coastline_ratio', 'Net_migration', 'Infant_mortality', 'GDP',\n",
    "               'Literacy', 'Phones_per_1000', 'Arable', 'Crops', 'Other', 'Climate',\n",
    "               'Birthrate', 'Deathrate', 'Agriculture', 'Industry', 'Service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma em float as strings numéricas\n",
    "for col in float_cols:\n",
    "    df[col] = [float(value) for value in df[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de regiões presentes no dataset\n",
    "\n",
    "regions = df['Region'].unique()\n",
    "regions.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pop_density'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantos países estão no 90º percentil?\n",
    "\n",
    "bins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "pop_bins = bins.fit_transform(df[['Pop_density']])\n",
    "(pop_bins >= 9).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantos one-hot-encodes são criados se encodadas as features Region e Climate\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "region_climate_encoded = encoder.fit_transform(df[['Region', 'Climate']].fillna(0))\n",
    "print(region_climate_encoded.shape)\n",
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q4 - Pipeline\n",
    "# Preencher int64 e float64 com as medianas\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                           ('scaler', StandardScaler())])\n",
    "\n",
    "cols_num = df.select_dtypes(include=['int64','float64']).columns\n",
    "cols_object = df.select_dtypes(exclude=['int64','float64']).columns\n",
    "pipeline.fit(df[cols_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Outliers\n",
    "\n",
    "df['Net_migration'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantil1, quantil3 = df['Net_migration'].quantile([.25, .75])\n",
    "quantil_interval = quantil3 - quantil1\n",
    "quantil_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interval = (quantil1 - 1.5 * quantil_interval, quantil3 + 1.5 * quantil_interval)\n",
    "interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_outliers = df['Net_migration'][df['Net_migration'] < interval[0]]\n",
    "high_outliers = df['Net_migration'][df['Net_migration'] > interval[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Outliers [Inf|Sup]\\n[{len(low_outliers)}|{len(high_outliers)}]\")\n",
    "print(f\"Total de outliers: {len(low_outliers)+len(high_outliers)}\")\n",
    "print(f\"Total de observações: {df.shape[0]}\")\n",
    "print(f\"Porcentagem de outliers: {(len(low_outliers)+len(high_outliers))/df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['sci.electronics', 'comp.graphics', 'rec.motorcycles']\n",
    "newsgroup = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "word_counts = count_vect.fit_transform(newsgroup.data)\n",
    "print(f\"Indice da palavra Phone: {count_vect.vocabulary_.get('phone')}\")\n",
    "print(f\"Ocorrencias: {word_counts[:-1, count_vect.vocabulary_.get('phone')].toarray().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(newsgroup.data)\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_matrix[:, count_vect.vocabulary_.get('phone')].toarray().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "# QUESTÕES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1\n",
    "\n",
    "Quais são as regiões (variável `Region`) presentes no _data set_? Retorne uma lista com as regiões únicas do _data set_ com os espaços à frente e atrás da string removidos (mas mantenha pontuação: ponto, hífen etc) e ordenadas em ordem alfabética."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1():\n",
    "    regions = df['Region'].unique()\n",
    "    regions.sort()\n",
    "    return list(regions)\n",
    "\n",
    "q1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2\n",
    "\n",
    "Discretizando a variável `Pop_density` em 10 intervalos com `KBinsDiscretizer`, seguindo o encode `ordinal` e estratégia `quantile`, quantos países se encontram acima do 90º percentil? Responda como um único escalar inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2():\n",
    "    bins = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "    pop_bins = bins.fit_transform(df[['Pop_density']])\n",
    "    return int((pop_bins >= 9).sum())\n",
    "\n",
    "q2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 3\n",
    "\n",
    "Se codificarmos as variáveis `Region` e `Climate` usando _one-hot encoding_, quantos novos atributos seriam criados? Responda como um único escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3():\n",
    "    encoder = OneHotEncoder()\n",
    "    region_climate_encoded = encoder.fit_transform(df[['Region', 'Climate']].fillna(0))\n",
    "    return int(region_climate_encoded.shape[1])\n",
    "\n",
    "q3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 4\n",
    "\n",
    "Aplique o seguinte _pipeline_:\n",
    "\n",
    "1. Preencha as variáveis do tipo `int64` e `float64` com suas respectivas medianas.\n",
    "2. Padronize essas variáveis.\n",
    "\n",
    "Após aplicado o _pipeline_ descrito acima aos dados (somente nas variáveis dos tipos especificados), aplique o mesmo _pipeline_ (ou `ColumnTransformer`) ao dado abaixo. Qual o valor da variável `Arable` após o _pipeline_? Responda como um único float arredondado para três casas decimais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_country = [\n",
    "    'Test Country', 'NEAR EAST', -0.19032480757326514,\n",
    "    -0.3232636124824411, -0.04421734470810142, -0.27528113360605316,\n",
    "    0.13255850810281325, -0.8054845935643491, 1.0119784924248225,\n",
    "    0.6189182532646624, 1.0074863283776458, 0.20239896852403538,\n",
    "    -0.043678728558593366, -0.13929748680369286, 1.3163604645710438,\n",
    "    -0.3699637766938669, -0.6149300604558857, -0.854369594993175,\n",
    "    0.263445277972641, 0.5712416961268142\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_country_df = pd.DataFrame([test_country], columns=df.columns)\n",
    "\n",
    "pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                           ('scaler', StandardScaler())])\n",
    "\n",
    "cols_num = df.select_dtypes(include=['int64','float64']).columns\n",
    "cols_object = df.select_dtypes(exclude=['int64','float64']).columns\n",
    "pipeline.fit(df[cols_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_country_transform = pipeline.transform(test_country_df[cols_num])\n",
    "test_country_transform = pd.DataFrame(test_country_transform, columns=cols_num)\n",
    "test_country_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(round(test_country_transform.Arable, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q4():\n",
    "    test_country_df = pd.DataFrame([test_country], columns=df.columns)\n",
    "\n",
    "    pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy=\"median\")),\n",
    "                               ('scaler', StandardScaler())])\n",
    "\n",
    "    cols_num = df.select_dtypes(include=['int64','float64']).columns\n",
    "    cols_object = df.select_dtypes(exclude=['int64','float64']).columns\n",
    "    pipeline.fit(df[cols_num])\n",
    "    test_country_transform = pipeline.transform(test_country_df[cols_num])\n",
    "    test_country_transform = pd.DataFrame(test_country_transform, columns=cols_num)\n",
    "    return float(round(test_country_transform.Arable, 3))\n",
    "\n",
    "q4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 5\n",
    "\n",
    "Descubra o número de _outliers_ da variável `Net_migration` segundo o método do _boxplot_, ou seja, usando a lógica:\n",
    "\n",
    "$$x \\notin [Q1 - 1.5 \\times \\text{IQR}, Q3 + 1.5 \\times \\text{IQR}] \\Rightarrow x \\text{ é outlier}$$\n",
    "\n",
    "que se encontram no grupo inferior e no grupo superior.\n",
    "\n",
    "Você deveria remover da análise as observações consideradas _outliers_ segundo esse método? Responda como uma tupla de três elementos `(outliers_abaixo, outliers_acima, removeria?)` ((int, int, bool))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q5():\n",
    "    quantil1, quantil3 = df['Net_migration'].quantile([.25, .75])\n",
    "    quantil_interval = quantil3 - quantil1\n",
    "    interval = (quantil1 - 1.5 * quantil_interval, quantil3 + 1.5 * quantil_interval)\n",
    "    low_outliers = df['Net_migration'][df['Net_migration'] < interval[0]]\n",
    "    high_outliers = df['Net_migration'][df['Net_migration'] > interval[1]]\n",
    "    return (len(low_outliers), len(high_outliers), False)\n",
    "\n",
    "q5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 6\n",
    "Para as questões 6 e 7 utilize a biblioteca `fetch_20newsgroups` de datasets de test do `sklearn`\n",
    "\n",
    "Considere carregar as seguintes categorias e o dataset `newsgroups`:\n",
    "\n",
    "```\n",
    "categories = ['sci.electronics', 'comp.graphics', 'rec.motorcycles']\n",
    "newsgroup = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)\n",
    "```\n",
    "\n",
    "\n",
    "Aplique `CountVectorizer` ao _data set_ `newsgroups` e descubra o número de vezes que a palavra _phone_ aparece no corpus. Responda como um único escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q6():\n",
    "    count_vect = CountVectorizer()\n",
    "    word_counts = count_vect.fit_transform(newsgroup.data)\n",
    "    return word_counts[:-1, count_vect.vocabulary_.get('phone')].toarray().sum()\n",
    "\n",
    "q6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 7\n",
    "\n",
    "Aplique `TfidfVectorizer` ao _data set_ `newsgroups` e descubra o TF-IDF da palavra _phone_. Responda como um único escalar arredondado para três casas decimais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q7():\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(newsgroup.data)\n",
    "    return float(round(tfidf_matrix[:, count_vect.vocabulary_.get('phone')].toarray().sum(), 3))\n",
    "\n",
    "q7()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
